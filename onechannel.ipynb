{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#un canale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NewLibraryENGcopia import *\n",
    "\n",
    "#POPULATION 1 (HEALTHY):\n",
    "name_data_st = '2019-01-23T16-06-32Pop1 healthy cortical .h5'\n",
    "name_data_BL = '2019-01-23T11-19-05PoP1 healthy cortical .h5'\n",
    "name_data_24 = '2019-01-24T16-00-33Pop1 24hour after.h5'\n",
    "complete_string='/Users/Gaia_1/Desktop/allh5files/healthy/healthy_baseline/'+name_data_BL\n",
    "#complete_string='/Users/Gaia_1/Desktop/allh5files/healthy/healthy_stimulation/'+name_data_st\n",
    "#complete_string='/Users/Gaia_1/Desktop/allh5files/healthy/healthy 24hrs later/'+name_data_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc177c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_poiproc(neurons,target,stim):\n",
    "    from scipy.stats import ks_2samp\n",
    "    dataframe = pd.DataFrame()\n",
    "    #counter_net=1\n",
    "    counter=0\n",
    "    #for net in list_dir_ok:\n",
    "    #print(counter_net,') ',net)\n",
    "    #counter_net+=1\n",
    "    list_neurons = neurons #np.genfromtxt(net, delimiter=',')\n",
    "    counter=0\n",
    "    print('Original number of neurons: ',len(list_neurons))\n",
    "    for neuron in tqdm(list_neurons):\n",
    "        neuron=neuron[neuron>0*10000]\n",
    "        neuron=neuron[neuron<200*10000]\n",
    "        print('  Neuron with ',neuron.shape[0],'spikes')\n",
    "        if neuron.shape[0]>1000:\n",
    "            \n",
    "            counter+=1\n",
    "        else:\n",
    "            print('    Excluded neuron with n spikes = ',neuron.shape[0])\n",
    "            continue\n",
    "        \n",
    "        ISI_healthy = np.diff(neuron)/10000\n",
    "        map_estimate = Bayesian_mixture_model(ISI_healthy)\n",
    "        map_estimate['Target']=int(target)\n",
    "        map_estimate['Stimulation']=int(stim)\n",
    "        df = pd.DataFrame.from_dict(map_estimate,orient='index')\n",
    "        dataframe = pd.concat([dataframe,df],axis = 1)\n",
    "    print('Final number of neurons: ',counter)\n",
    "    print('Target = ',target)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "    #ks_2samp(lista_samples,ISI_healthy,mode = 'asymp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de216378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_spike_sorting(name_data,complete_string,threshold,clustering,coeff,c1):\n",
    "    #file reading:\n",
    "    data = h5py.File(complete_string,'r')\n",
    "    data_readings = data['Data']['Recording_0']['AnalogStream']['Stream_0']['ChannelData'][()]\n",
    "    info = data['Data']['Recording_0']['AnalogStream']['Stream_0']['InfoChannel'][()]\n",
    "    info_table = pd.DataFrame(info, columns = list(info.dtype.fields.keys()))\n",
    "    labels = info_table['Label']\n",
    "    readings = pd.DataFrame(data = data_readings.transpose(), columns = labels)\n",
    "    fs = 10000 #Sampling Frequency\n",
    "    print('data shape: ',readings.shape)\n",
    "    prova=readings.drop([b'Ref'],axis=1)\n",
    "    #prova=prova.iloc[inizio:fine, :10]\n",
    "    prova=prova.iloc[:, :10]\n",
    "    ref=readings[b'Ref']\n",
    "    #ref=ref[inizio:fine]\n",
    "    #filtering:\n",
    "    prova_rows = range(prova.shape[0])\n",
    "    filt_prova = pd.DataFrame(data = 0, columns=prova.columns, index=prova_rows, dtype = \"float32\")\n",
    "    lowcut = 300\n",
    "    highcut = 3000\n",
    "    fs=10000\n",
    "    order=8\n",
    "    b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "    filt_ref=filtfilt(b,a,ref)\n",
    "    for x in tqdm(range(prova.shape[1])):\n",
    "        filt_prova.values[:,x] = scipy.signal.filtfilt(b, a, prova.values[:,x])\n",
    "    for electrode in prova.columns:\n",
    "        filt_prova[electrode] = filt_prova[electrode] - filt_ref\n",
    "    prova=filt_prova\n",
    "    #detection:\n",
    "    if threshold==0:\n",
    "        threshold=[]\n",
    "        for i,electrode in tqdm(enumerate(prova.columns)):\n",
    "            threshold.append(coeff*(scipy.stats.median_abs_deviation(prova[electrode].values,scale='normal')))            \n",
    "    all_ind=[]\n",
    "    for i,electrode in tqdm(enumerate(prova.columns)):\n",
    "        channel=prova[electrode]\n",
    "        thresh=threshold[i]\n",
    "        ind=find_all_spikes(channel,thresh)\n",
    "        all_ind.append(ind)\n",
    "    #spike extraction:\n",
    "    cut_outs=[]\n",
    "    all_new=[]\n",
    "    for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "        ind=all_ind[i]\n",
    "        channel=prova[electrode]\n",
    "        cut_outs1,all_new1=cut_all(ind,channel,c1)\n",
    "        cut_outs.append(cut_outs1)\n",
    "        all_new.append(all_new1)    \n",
    "    # Clustering:\n",
    "    final_data=[]\n",
    "    if clustering=='kmeans':\n",
    "            for channel in (tqdm(range(len(cut_outs)))):\n",
    "                #channel_clusters1=comparative_clus(cut_outs[channel],all_new[channel],prova.iloc[:,channel])\n",
    "                channel_clusters1=clus(cut_outs[channel],all_new[channel],prova.iloc[:,channel])\n",
    "                final_data.append(channel_clusters1)\n",
    "    elif clustering=='dbscan':\n",
    "            for channel in (tqdm(range(len(cut_outs)))):\n",
    "                eps=int(scipy.stats.median_abs_deviation(prova.iloc[:,channel])/2)\n",
    "                channel_clusters1=dbscan_clustering(cut_outs[channel],all_new[channel],prova.iloc[:,channel],eps)\n",
    "                final_data.append(channel_clusters1)\n",
    "    neurons=[]\n",
    "    for channel in final_data:\n",
    "        for neuron in channel:\n",
    "            neurons.append(neuron)\n",
    "    print(len(neurons),' neurons detected and sorted')\n",
    "    adj_neur=[]\n",
    "    counter = 0\n",
    "    max_len=0\n",
    "    for neuron in neurons:\n",
    "        print('counter: ',counter,neuron.shape[0])\n",
    "        if neuron.shape[0]>max_len:\n",
    "            max_len=neuron.shape[0]\n",
    "        counter+=1\n",
    "    for neuron in neurons:\n",
    "        if neuron.shape[0]<max_len and neuron.shape[0]>=1000:\n",
    "            diff = max_len-neuron.shape[0]\n",
    "            adj_neur.append(np.concatenate((neuron,np.zeros([diff]))))\n",
    "    save_data = 'After'+name_data+'.txt'\n",
    "    np.savetxt(\"/Users/Gaia_1/Desktop/tesi/Data after SS prova/%s.txt\" % save_data,adj_neur, delimiter=', ', fmt='%12.8f')\n",
    "    print(save_data)\n",
    "    return neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e286aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File(complete_string,'r')\n",
    "\n",
    "data_readings = data['Data']['Recording_0']['AnalogStream']['Stream_0']['ChannelData'][()]\n",
    "info = data['Data']['Recording_0']['AnalogStream']['Stream_0']['InfoChannel'][()]\n",
    "info_table = pd.DataFrame(info, columns = list(info.dtype.fields.keys()))\n",
    "labels = info_table['Label']\n",
    "readings = pd.DataFrame(data = data_readings.transpose(), columns = labels)\n",
    "fs = 10000 #Sampling Frequency\n",
    "print(readings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "inizio=0\n",
    "len_data=180000 #192 secondi\n",
    "len_data=len(readings)\n",
    "prova_multi=readings.iloc[inizio:len_data, :59]\n",
    "#prova=prova.drop([b'Ref'],axis=1)\n",
    "prova=prova_multi[b'33']\n",
    "ref=readings[b'Ref']\n",
    "ref=ref[inizio:len_data]\n",
    "#prova=readings.iloc[:150000, :5]\n",
    "print(prova.shape,ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcut = 300\n",
    "highcut = 3000\n",
    "fs=10000\n",
    "order=8\n",
    "b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "filt_ref=filtfilt(b,a,ref)\n",
    "filt_prova=filtfilt(b,a,prova)\n",
    "f_prova = filt_prova - filt_ref\n",
    "prova=f_prova\n",
    "prova.shape\n",
    "#r_prova=prova.reshape(-1, 1)\n",
    "#scaler = StandardScaler()\n",
    "#s_prova = scaler.fit_transform(r_prova)\n",
    "#prova=s_prova\n",
    "#s_prova.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44af205",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=4*(scipy.stats.median_abs_deviation(prova,scale='normal'))\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed210bb",
   "metadata": {},
   "source": [
    "# Spike detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aecf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=find_all_spikes(prova,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa623c59",
   "metadata": {},
   "source": [
    "pos, neg, alls=find_all_spikes(prova,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de52d5",
   "metadata": {},
   "source": [
    "BL:            \n",
    "THRESH=4: fr 131 Hz (15014 pos, 40487 neg)           \n",
    "THRESH=3: fr 390 Hz (72800 pos, 91948 neg)           \n",
    "stim:     \n",
    "THRESH=4: fr 14 Hz (2793 pos, 3261 neg)           \n",
    "THRESH=3: fr 142.14 Hz (30101 pos, 30450 neg)       \n",
    "24hrs:    \n",
    "THRESH=4: fr 556 Hz (85496 pos, 149916 neg)            \n",
    "THRESH=3: fr 1092.19 Hz (220928 pos, 240849 neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de9c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19ec8cc4",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(30, 15))\n",
    "plt.plot(prova, label='Signal Data')\n",
    "plt.scatter(pos, [prova[i] for i in pos], c='red', marker='o', label='Local Maxima')\n",
    "plt.scatter(neg, [prova[i] for i in neg], c='green', marker='o', label='Local Minima')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Signal Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Signal Data with Detected Spikes')\n",
    "#plt.axis([0,3000,-5,5])\n",
    "#plt.savefig('spikes1.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddba0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b355436f",
   "metadata": {},
   "source": [
    "minima,maxima=RMM(prova)\n",
    "print(len(maxima))\n",
    "#segnale BASELINE: firing rate=70 (spikes 26148)\n",
    "#segnale Stimulation: firing rate=69 (spikes 32980)\n",
    "#segnale KA stimulation: firing rate=81 (spikes 47860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a7067",
   "metadata": {},
   "source": [
    "minima,maxima=find_spikes(prova)\n",
    "len(maxima)\n",
    "#segnale BASELINE: firing rate=27 (spikes 10276)\n",
    "#segnale Stimulation: firing rate=27 (spikes 13198)\n",
    "#segnale KA stimulation: firing rate=32 (spikes 19178)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256bf0b",
   "metadata": {},
   "source": [
    "minima,maxima=find_spikes_with_memory(prova)\n",
    "len(maxima)\n",
    "#segnale BASELINE: firing rate=25 (spikes 9270)\n",
    "#segnale Stimulation: firing rate=22 (spikes 10579)\n",
    "#segnale KA stimulation: firing rate=8 (spikes 5054)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246ac0b",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(30, 15))\n",
    "plt.plot(prova, label='Signal Data')\n",
    "plt.scatter(neg, [prova[i] for i in neg], c='red', marker='o', label='Local Minima')\n",
    "plt.scatter(pos, [prova[i] for i in pos], c='green', marker='x', label='Local maxima')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Signal Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Signal Data with Detected Spikes')\n",
    "#plt.axis([0,30000,-5,5])\n",
    "#plt.savefig('spikes2.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f5432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc55e4f8",
   "metadata": {},
   "source": [
    "# Cutouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca734757",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=2\n",
    "cut_outs,alls=cut_all(ind,prova,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8428d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a2cc5fa",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "c1=1\n",
    "pos_cut,n_pos, neg_cut,n_neg = cut(pos,neg,prova,c1)\n",
    "#savedp = copy.deepcopy(pos_cut)\n",
    "#savedn = copy.deepcopy(neg_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a48b8b",
   "metadata": {},
   "source": [
    "pos_cut=mask_cuts(pos_cut)\n",
    "neg_cut= mask_cuts(neg_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91195eaa",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "x=randint(1,min(len(pos_cut),len(neg_cut)))\n",
    "\n",
    "plt.plot(savedp[x])\n",
    "plt.plot(pos_cut[x])\n",
    "plt.show()\n",
    "#plt.plot(savedn[x])\n",
    "#plt.plot(neg_cut[x])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabeff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dba03b7",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "pca = PCA()\n",
    "pca.fit(pos_cut)\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "explained_variance_df = pd.DataFrame(data={'Explained Variance': explained_variances},\n",
    "                                     index=range(1, len(explained_variances) + 1))\n",
    "explained_variance_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67be79a",
   "metadata": {},
   "source": [
    "cumulative_explained_variance = np.cumsum(explained_variances)\n",
    "cumulative_explained_variance_df = pd.DataFrame(data={'Cumulative Explained Variance': cumulative_explained_variance},\n",
    "                                                index=range(1, len(cumulative_explained_variance) + 1))\n",
    "cumulative_explained_variance_df.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ef327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0ea887",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8990ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=[]\n",
    "final_data.append(selective_clus(cut,alls,prova))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85eb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[]\n",
    "for neuron in final_data:\n",
    "    neurons.append(neuron[0])\n",
    "len(neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ec42d",
   "metadata": {},
   "source": [
    "• distance tra indici detettati pari a lunghezza spike\n",
    "• poi clustering con range dda 2 a 3 e se silhouette sotto una soglia è un cluster solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9764a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68974f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdc89d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#x=randint(0,len(neurons)-1)\n",
    "for x in range(0,len(neurons)):\n",
    "    print('neur: ',x,len(neurons[x]))\n",
    "    data_healthy=neurons[x]\n",
    "    ISI_healthy = np.diff(data_healthy)/10000\n",
    "    plt.hist(ISI_healthy, bins=100, density=False, alpha=0.5, color='blue', edgecolor='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a524c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_neur=[]\n",
    "counter = 0\n",
    "max_len=0\n",
    "\n",
    "for neu in neurons:\n",
    "    print('counter: ',counter,neu.shape[0])\n",
    "    if neu.shape[0]>max_len:\n",
    "        max_len=neu.shape[0]\n",
    "    counter+=1\n",
    "for neuron in neurons:\n",
    "    if neuron.shape[0]<=max_len:\n",
    "        diff = max_len-neuron.shape[0]\n",
    "        adj_neur.append(np.concatenate((neuron,np.zeros([diff]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf70e2",
   "metadata": {},
   "source": [
    "x=randint(0,len(adj_neur)-1)\n",
    "print('neur: ',x)\n",
    "data_healthy=adj_neur[x]\n",
    "ISI_healthy = np.diff(data_healthy)/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52055d2c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "locs_diff=np.diff(data_healthy)\n",
    "#plt.axis([-3,500,0,0.05])\n",
    "plt.hist(locs_diff, bins=50, density=False, alpha=0.5, color='blue', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac710f3",
   "metadata": {},
   "source": [
    "## Point Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fe373",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISI_data=ISI_healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75283992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_estimate,ppc_trace= Bayesian_mixture_model(ISI_healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe8561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with model:\n",
    "#    ppc_trace = pm.sample_posterior_predictive(trace,model=model)\n",
    "def trace(ppc_trace):\n",
    "    bins = np.arange(0, .5, 1e-3) \n",
    "    plt.figure (figsize=(14,10))\n",
    "\n",
    "    hist = np.histogram(ppc_trace['posterior_predictive']['like'].values,bins=bins)\n",
    "    #plt.axis([-0.01,0.13,0,160])\n",
    "    a= plt.hist(ISI_healthy,bins)\n",
    "    plt.plot(hist[1][:-1],hist[0]/1000,linewidth=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_samples=[]\n",
    "for i in list(ppc_trace['posterior_predictive']['like'].values):\n",
    "    lista_samples.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87bf175",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, _ = np.histogram(ISI_healthy, bins) \n",
    "prob_emp = counts / np.sum(counts)\n",
    "counts, _ = np.histogram(lista_samples, bins) \n",
    "prob_model = counts / np.sum(counts)\n",
    "\n",
    "Femp = np.cumsum(prob_emp)           \n",
    "Fmodel = np.cumsum(prob_model)          \n",
    "plt.figure()\n",
    "plt.plot(bins[:-1], Femp)                \n",
    "plt.plot(bins[:-1], Fmodel, 'r')       \n",
    "#plt.xlim([0, 0.2])                  \n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend(['Empirical','Model'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc155ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "Nlow = len(ISI_healthy)  \n",
    "# Plot the confidence bounds\n",
    "plt.plot([0, 1], [x + 1.36 / np.sqrt(Nlow) for x in [0, 1]], 'r:')\n",
    "plt.plot([0, 1], [x - 1.36 / np.sqrt(Nlow) for x in [0, 1]], 'r:')\n",
    "plt.plot(Femp, Fmodel)\n",
    "#plt.axis([0, 1, 0, 1])         \n",
    "plt.xlabel('Model CDF')\n",
    "plt.ylabel('Empirical CDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcdacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_Bayesian_mixture_model(ISI_data):\n",
    "    with pm.Model() as model:\n",
    "        ##### WALD DISTRIBUTION (INVERSE GAUSSIAN)\n",
    "        mu1 = pm.Uniform('mu1',lower=0.01,upper=0.1)\n",
    "        lam1 = pm.Uniform('lam1',lower=0.001,upper=0.04)\n",
    "        obs1 = pm.Wald.dist(mu=mu1,lam=lam1)\n",
    "\n",
    "\n",
    "        mu2 = pm.Uniform('mu2',lower=0,upper=0.2)\n",
    "        sigma2 = pm.Uniform('sigma2',lower=0.0001,upper=0.5)\n",
    "        obs2 = pm.TruncatedNormal.dist(mu=mu2, sigma=sigma2, lower=0.0)\n",
    "\n",
    "        mu3 = pm.Uniform('mu3',lower=0.1,upper=0.6)\n",
    "        sigma3 = pm.Uniform('sigma3',lower=0.0001,upper=0.5)\n",
    "        obs3 = pm.TruncatedNormal.dist(mu=mu3, sigma=sigma3, lower=0.0)\n",
    "\n",
    "\n",
    "        w = pm.Dirichlet('w', a=np.array([1., .4, .4]))\n",
    "        #w = pm.Dirichlet('w', a=np.array([1., .4]))\n",
    "\n",
    "        like = pm.Mixture('like', w=w, comp_dists = [obs1, obs2, obs3], observed=ISI_data)\n",
    "        #like = pm.Mixture('like', w=w, comp_dists = [obs1, obs2], observed=ISI_data)\n",
    "\n",
    "        step = pm.NUTS(target_accept=0.9)\n",
    "        trace = pm.sample(step=step,draws=1000,chains=1,tune=1000,cores=4)\n",
    "        \n",
    "        ppc_trace = pm.sample_posterior_predictive(trace,model=model)\n",
    "        if ppc_trace==0:\n",
    "            print('ppc_trace not succesful')\n",
    "        lista_samples=[]\n",
    "        for i in list(ppc_trace['posterior_predictive']['like'].values):\n",
    "            lista_samples.extend(i)\n",
    "        counts, _ = np.histogram(ISI_healthy, bins) \n",
    "        prob_emp = counts / np.sum(counts)\n",
    "        counts, _ = np.histogram(lista_samples, bins) \n",
    "        prob_model = counts / np.sum(counts)\n",
    "\n",
    "        Femp = np.cumsum(prob_emp)           \n",
    "        Fmodel = np.cumsum(prob_model)          \n",
    "        plt.figure()\n",
    "        plt.plot(bins[:-1], Femp)                \n",
    "        plt.plot(bins[:-1], Fmodel, 'r')       \n",
    "        #plt.xlim([0, 0.2])                  \n",
    "        plt.xlabel('Time [s]')\n",
    "        plt.ylabel('CDF')\n",
    "        plt.legend(['Empirical','Model'])\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "        Nlow = len(ISI_healthy)  \n",
    "        # Plot the confidence bounds\n",
    "        plt.plot([0, 1], [x + 1.36 / np.sqrt(Nlow) for x in [0, 1]], 'r:')\n",
    "        plt.plot([0, 1], [x - 1.36 / np.sqrt(Nlow) for x in [0, 1]], 'r:')\n",
    "        plt.plot(Femp, Fmodel)\n",
    "        #plt.axis([0, 1, 0, 1])         \n",
    "        plt.xlabel('Model CDF')\n",
    "        plt.ylabel('Empirical CDF')\n",
    "        plt.show()\n",
    "        \n",
    "    map_estimate = pm.find_MAP(model=model)\n",
    "    \n",
    "    del map_estimate['w_simplex__']\n",
    "    del map_estimate['mu1_interval__']\n",
    "    del map_estimate['lam1_interval__']\n",
    "    del map_estimate['mu2_interval__']\n",
    "    del map_estimate['sigma2_interval__']\n",
    "    del map_estimate['mu3_interval__']\n",
    "    del map_estimate['sigma3_interval__']\n",
    "    \n",
    "    map_estimate['w1'] = map_estimate['w'][0]\n",
    "    map_estimate['w2'] = map_estimate['w'][1]\n",
    "    map_estimate['w3'] = map_estimate['w'][2]\n",
    "\n",
    "    del map_estimate['w']\n",
    "\n",
    "\n",
    "    return map_estimate, ppc_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ee18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame()\n",
    "list_neurons = neurons\n",
    "counter=0\n",
    "target=0\n",
    "print('Original number of neurons: ',len(list_neurons))\n",
    "for neuron in list_neurons:\n",
    "    neuron=neuron[neuron>0*10000]\n",
    "    neuron=neuron[neuron<200*10000]\n",
    "    print('  Neuron with ',neuron.shape[0],'spikes')\n",
    "    if neuron.shape[0]>1000:\n",
    "\n",
    "        counter+=1\n",
    "    else:\n",
    "        print('    Excluded neuron with n spikes = ',neuron.shape[0])\n",
    "        continue\n",
    "\n",
    "    ISI_healthy = np.diff(neuron)/10000\n",
    "\n",
    "\n",
    "    map_estimate,ppc_trace = this_Bayesian_mixture_model(ISI_healthy)\n",
    "    trace(ppc_trace)\n",
    "    df = pd.DataFrame.from_dict(map_estimate,orient='index')\n",
    "    dataframe = pd.concat([dataframe,df],axis = 1)\n",
    "print('Final number of neurons: ',counter)\n",
    "print('Target = ',target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752738c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = dataframe.T\n",
    "final.to_csv('Data after PP/DataAfter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55272c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5e0cbdd",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6920ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_visualizer(trials_obj,n_models,choice=False,**choice_var):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    performance = [1-t['result']['loss'] for t in trials_obj.trials]\n",
    "    \n",
    "    \n",
    "    hyperparam= list(trials_obj.trials[0]['misc']['vals'].keys())\n",
    "    \n",
    "    values_dict ={}\n",
    "    \n",
    "    for i in hyperparam:\n",
    "        \n",
    "        values_dict[i]=[]\n",
    "        \n",
    "        for j in trials_obj.trials:\n",
    "            \n",
    "            if(len(j['misc']['vals'][i])==0):\n",
    "                \n",
    "                values_dict[i].append(np.NaN)\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                values_dict[i].append(j['misc']['vals'][i][0])\n",
    "                \n",
    "    out = pd.DataFrame.from_dict(values_dict)\n",
    "    \n",
    "    out['performance'] = performance\n",
    "    \n",
    "    out=out.sort_values(by=['performance'])\n",
    "    \n",
    "    \n",
    "    if choice:\n",
    "        \n",
    "        for i in list(choice_var.keys()):\n",
    "        \n",
    "            for j,_ in enumerate(choice_var[i]):\n",
    "        \n",
    "                out[i]=out[i].replace(j,choice_var[i][j])\n",
    "    \n",
    "    return out.tail(n_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d895d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data after PP/Data')\n",
    "dataset = dataset.drop(['Unnamed: 0'],axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb252cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

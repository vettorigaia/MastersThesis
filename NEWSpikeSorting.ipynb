{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfcc3c9",
   "metadata": {},
   "source": [
    "# Spike sorting with RMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provare silhouette threshold =0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa13282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "list_dir=glob.glob(\"/Users/Gaia_1/Desktop/allh5filestutto/*.h5\")\n",
    "for file in list_dir:\n",
    "    print(file)\n",
    "print(len(list_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cff87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#final_neurons=[]\n",
    "#final_firing=[]\n",
    "from NewLibraryENGcopia import *\n",
    "import glob\n",
    "#final=[]\n",
    "list_dir=glob.glob(\"/Users/Gaia_1/Desktop/allh5filestutto/*.h5\")\n",
    "output_path='/Users/Gaia_1/Desktop/Data after SS thresh 3'\n",
    "for file in tqdm(list_dir):\n",
    "    target=1\n",
    "    stim=0\n",
    "    if 'health' in file:\n",
    "        target=0\n",
    "    if 'after' in file:\n",
    "        stim=1\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    print(file_name,':','target',target,'stimulation',stim)\n",
    "    neurons,firing=this_spike_sorting(file,output_path)\n",
    "    final_neurons.append(neurons)\n",
    "    final_firing.append(firing)\n",
    "    print(firing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801787d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543b7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f889344",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store numerical values\n",
    "healthy_values_BL = []\n",
    "healthy_values_af = []\n",
    "\n",
    "lrrk2_values_BL = []\n",
    "lrrk2_values_af = []\n",
    "\n",
    "# Iterate through the list of lists\n",
    "for firing in final_firing:\n",
    "    file_name = firing[0]\n",
    "    numerical_values = [float(arr[0]) for arr in firing[1:]]\n",
    "    \n",
    "    # Check if the file name contains 'after'\n",
    "    if 'healthy' in file_name:\n",
    "        if 'after' in file_name:\n",
    "            healthy_values_af.extend(numerical_values)\n",
    "        else:\n",
    "            healthy_values_BL.extend(numerical_values)\n",
    "    else:\n",
    "        if 'after' in file_name:\n",
    "            lrrk2_values_af.extend(numerical_values)\n",
    "        else:\n",
    "            lrrk2_values_BL.extend(numerical_values)\n",
    "print('HEALTHY BL mean = ', np.mean(healthy_values_BL))\n",
    "print('HEALTHY AFTER mean = ', np.mean(healthy_values_af))\n",
    "print('LRRK2 BL mean = ', np.mean(lrrk2_values_BL))\n",
    "print('LRRK2 AFTER mean = ', np.mean(lrrk2_values_af))\n",
    "# Create box plots for each group\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.boxplot([healthy_values_BL,healthy_values_af, lrrk2_values_BL, lrrk2_values_af], labels=['Healthy BL','Healthy after', 'LRRK2 BL','LRRK2 after'])\n",
    "plt.title('Distribution of Firing Rates')\n",
    "plt.ylabel('Firing Rate')\n",
    "#plt.savefig('firing_rates_boxplot_sm.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb2899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "284e35bd",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c68ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [firing[0] for firing in final_firing]\n",
    "numerical_values = [[float(arr[0]) for arr in firing[1:]] for firing in final_firing]\n",
    "\n",
    "# Save the file names and numerical values to a text file\n",
    "output_path = \"/Users/Gaia_1/Desktop\"\n",
    "with open(f\"{output_path}/firings.txt\", \"w\") as file:\n",
    "    for file_name, values in zip(file_names, numerical_values):\n",
    "        file.write(f\"{file_name}: {', '.join(map(str, values))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_spike_sorting(input_path,output_path):\n",
    "    name_data = input_path.split(\"/\")[-1]\n",
    "    #file reading:\n",
    "    print('File Reading...')\n",
    "    data = h5py.File(input_path,'r')\n",
    "    data_readings = data['Data']['Recording_0']['AnalogStream']['Stream_0']['ChannelData'][()]\n",
    "    info = data['Data']['Recording_0']['AnalogStream']['Stream_0']['InfoChannel'][()]\n",
    "    info_table = pd.DataFrame(info, columns = list(info.dtype.fields.keys()))\n",
    "    labels = info_table['Label']\n",
    "    readings = pd.DataFrame(data = data_readings.transpose(), columns = labels)\n",
    "    fs = 10000 #Sampling Frequency\n",
    "    print('data shape: ',readings.shape)\n",
    "    prova=readings.drop([b'Ref'],axis=1)\n",
    "    #prova=prova.iloc[0:750500, :]\n",
    "    #prova=prova.iloc[:, :15]\n",
    "    ref=readings[b'Ref']\n",
    "    #ref=ref[0:750500]\n",
    "    freqs, spectrogram = signal.welch(readings[b'Ref'].values, fs=10000, nfft=1024)\n",
    "    noise_freq = freqs[spectrogram.argmax()]\n",
    "    Q = 30\n",
    "    b, a = scipy.signal.iirnotch(noise_freq, Q, fs)\n",
    "    Q = 60\n",
    "    b_2, a_2 = scipy.signal.iirnotch(2*noise_freq, Q, fs)\n",
    "    channel = readings[b'Ref'].values\n",
    "    pre_filtered_ref = scipy.signal.filtfilt(b, a, channel)\n",
    "    pre_filtered_ref = scipy.signal.filtfilt(b_2, a_2, pre_filtered_ref) \n",
    "    ref=pre_filtered_ref\n",
    "\n",
    "    #filtering:\n",
    "    prova_rows = range(prova.shape[0])\n",
    "    filt_prova = pd.DataFrame(data = 0, columns=prova.columns, index=prova_rows, dtype = \"float32\")\n",
    "    lowcut = 300\n",
    "    highcut = 3000\n",
    "    fs=10000\n",
    "    order=8\n",
    "    b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "    filt_ref=filtfilt(b,a,ref)\n",
    "    print('Data Filtering:')\n",
    "    for x in tqdm(range(prova.shape[1])):\n",
    "        filt_prova.values[:,x] = scipy.signal.filtfilt(b, a, prova.values[:,x])\n",
    "    for electrode in prova.columns:\n",
    "        filt_prova[electrode] = filt_prova[electrode] - filt_ref\n",
    "    prova=filt_prova\n",
    "    #detection:\n",
    "    all_ind=[]\n",
    "    print('Spike Detection: ')\n",
    "    for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "        channel=prova[electrode]\n",
    "        #ind=windowed_spike_detection(channel)\n",
    "        ind=this_spike_detection(channel)\n",
    "        all_ind.append(ind)\n",
    "    #spike extraction:\n",
    "    cut_outs=[]\n",
    "    all_new=[]\n",
    "    print('Spike extraction: ')\n",
    "    for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "        ind=all_ind[i]\n",
    "        channel=prova[electrode]\n",
    "        cut_outs1,all_new1=this_new_cut(ind,channel)\n",
    "        cut_outs.append(cut_outs1)\n",
    "        all_new.append(all_new1)    \n",
    "    # Clustering:\n",
    "    final_data=[]\n",
    "    final_firing=[]\n",
    "    final_firing.append(name_data)\n",
    "    print('Clustering: ')\n",
    "    for channel in (tqdm(range(len(cut_outs)))):\n",
    "        channel_clusters1,final_firing1=this_clus(cut_outs[channel],all_new[channel],prova.iloc[:,channel])\n",
    "        final_data.append(channel_clusters1)\n",
    "        final_firing.append(final_firing1)\n",
    "    neurons=[]\n",
    "    for channel in final_data:\n",
    "        for neuron in channel:\n",
    "            neurons.append(neuron)\n",
    "    print(len(neurons),' neurons detected and sorted')\n",
    "    adj_neur=[]\n",
    "    counter = 0\n",
    "    max_len=0\n",
    "    for neuron in neurons:\n",
    "        print('counter: ',counter,neuron.shape[0])\n",
    "        if neuron.shape[0]>max_len:\n",
    "            max_len=neuron.shape[0]\n",
    "        counter+=1\n",
    "    for neuron in neurons:\n",
    "        if neuron.shape[0]<max_len:\n",
    "            diff = max_len-neuron.shape[0]\n",
    "            adj_neur.append(np.concatenate((neuron,np.zeros([diff]))))\n",
    "    save_data = 'After'+name_data+'.txt'\n",
    "    #np.savetxt(\"/Users/Gaia_1/Desktop/tesi/Data after SS/%s.txt\" % save_data,adj_neur, delimiter=', ', fmt='%12.8f')\n",
    "    np.savetxt(f\"{output_path}/{save_data}.txt\", adj_neur, delimiter=', ', fmt='%12.8f')\n",
    "\n",
    "    print('saved: ',save_data)\n",
    "    return neurons, final_firing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_spike_detection(data):\n",
    "    spike_length=30 #3ms (0.003s)\n",
    "    window_length=600000 #60 sec (1min)\n",
    "    neg_data=-(data)\n",
    "    abs_data=abs(data)\n",
    "    i=0\n",
    "    ind=[]\n",
    "    while i < len(data)-window_length:\n",
    "        neg_window=neg_data[i:i+window_length]\n",
    "        abs_window=abs_data[i:i+window_length]\n",
    "        window=data[i:i+window_length]\n",
    "        #coeff=4\n",
    "        #if abso==0:\n",
    "        coeff=3\n",
    "        thresh=coeff*(scipy.stats.median_abs_deviation(window,scale='normal'))\n",
    "        #else:\n",
    "            #coeff=4\n",
    "            #thresh=coeff*(scipy.stats.median_abs_deviation(abs_window,scale='normal'))\n",
    "        ind1, peaks =find_peaks(neg_window, height=thresh,distance=spike_length)\n",
    "        del peaks\n",
    "        last=i\n",
    "        if len(ind1):\n",
    "            last=i+ind1[-1]\n",
    "        ind.extend([index + i for index in ind1])\n",
    "        i=last+spike_length #0.003 s (30ms)\n",
    "    firing_rate=len(ind)*10000/len(data)\n",
    "    print(len(ind), ' spikes detected;  ', 'firing rate: {:.2f}'.format(firing_rate),'Hz')\n",
    "    return ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_new_cut(alls,data):\n",
    "    pre = 0.001\n",
    "    post = 0.002\n",
    "    fs=10000\n",
    "    prima = int(pre*fs)\n",
    "    dopo = int(post*fs)\n",
    "    lunghezza_indici = len(alls)\n",
    "    cut= np.empty([lunghezza_indici, prima+dopo])\n",
    "    dim = data.shape[0]\n",
    "    k=0\n",
    "    coeff=1.5\n",
    "    signal_std=np.std(data)\n",
    "    signal_mean=np.mean(data)\n",
    "    standard_mean=signal_mean\n",
    "    standard_threshold=signal_std\n",
    "    for i in alls:\n",
    "        if (i-prima >= 0) and (i+dopo <= dim):\n",
    "            spike= data[(int(i)-prima):(int(i)+dopo)].squeeze()\n",
    "            cut[k,:] = spike\n",
    "            k+=1\n",
    "    standards=np.std(cut,axis=1)\n",
    "    means=np.mean(cut,axis=1)\n",
    "\n",
    "    thr1=coeff*standard_threshold\n",
    "    thr2=3*standard_mean\n",
    "    \n",
    "    indices=np.where((standards<thr1)&(means<thr2))[0]\n",
    "\n",
    "    filtered_alls = np.array(alls)[indices]\n",
    "    filtered_cut=cut[indices]\n",
    "    \n",
    "    spike_means=np.mean(filtered_cut,axis=1,keepdims=True)\n",
    "    spike_stds=np.std(filtered_cut,axis=1,keepdims=True)\n",
    "    \n",
    "    standardized_cuts=(filtered_cut-spike_means)/spike_stds\n",
    "    \n",
    "    firing_rate=len(indices)*10000/len(data)\n",
    "    print(len(alls)-len(indices),' spikes removed;  ', 'firing rate: {:.2f}'.format(firing_rate),'Hz')\n",
    "    return standardized_cuts,filtered_alls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052dc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_clus(cut,spike_list,data):\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    import numpy as np\n",
    "    scale = StandardScaler()\n",
    "    estratti_norm = scale.fit_transform(cut)\n",
    "    print('\\n______________________________________________________________________________________________________________')\n",
    "    print('Total spikes: ', estratti_norm.shape[0])\n",
    "    n_comp=3\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    transformed = pca.fit_transform(estratti_norm)\n",
    "    spike_list=np.array(spike_list)\n",
    "    kmeans_score=[]\n",
    "    final_data=[]\n",
    "    final_firing=[]\n",
    "    for n in range (2,4):\n",
    "        model = KMeans(n_clusters=n, init='k-means++', n_init=10, max_iter=400, tol=0.25, verbose=0, random_state=None, copy_x=True,  algorithm='lloyd')\n",
    "        labels = model.fit_predict(transformed)\n",
    "        silhouette_avg = silhouette_score(transformed, labels)\n",
    "        kmeans_score.append(silhouette_avg)\n",
    "    top_clusters_kmeans = kmeans_score.index(max(kmeans_score))+2\n",
    "    if max(kmeans_score)>=0.4:\n",
    "        print(\"\\n\\n\\033[1;31;47mBest cluster in the range 1 to 3: \",top_clusters_kmeans,\", with a silhouette score of: \",max(kmeans_score), \"\\u001b[0m  \\n\\n\")\n",
    "        model = KMeans(n_clusters=top_clusters_kmeans,  init='k-means++', n_init=10, max_iter=400, tol=0.25, verbose=0, random_state=None, copy_x=True,  algorithm='lloyd')\n",
    "        labels = model.fit_predict(transformed)\n",
    "    else:\n",
    "        print('Clustering algorithm detected only one cluster')\n",
    "        labels=np.zeros(len(spike_list),dtype=int)\n",
    "    unique_labels=np.unique(labels)\n",
    "    firings=np.zeros(len(unique_labels))\n",
    "    color=[]\n",
    "    for i in labels:\n",
    "        color.append(plt.rcParams['axes.prop_cycle'].by_key()['color'][i])\n",
    "    for i,cluster_label in enumerate(unique_labels):\n",
    "        cluster_data=cut[labels==cluster_label]\n",
    "        mean_wave=np.mean(cluster_data, axis=0)\n",
    "        std_wave=np.std(cluster_data, axis=0)\n",
    "        \n",
    "        filtered_cluster_data=cluster_data\n",
    "        \n",
    "        #plotting_data=filtered_cluster_data.transpose()\n",
    "        \n",
    "        firings[i]=len(filtered_cluster_data)*10000/len(data)\n",
    "        \n",
    "        fig = plt.figure(figsize=(8,10))\n",
    "        \n",
    "        plt.subplot(3,1,i+1)\n",
    "        #plt.plot(plotting_data,alpha=0.5)\n",
    "        \n",
    "        plt.title(f'Cluster {i} \\n numerosity: {len(filtered_cluster_data)}')\n",
    "        plt.xlabel('Time [ms]')\n",
    "        plt.ylabel('Signal Amplitude')\n",
    "        mean_wave = np.mean(filtered_cluster_data, axis=0)\n",
    "        std_wave = np.std(filtered_cluster_data, axis=0)\n",
    "        plt.errorbar(range(mean_wave.shape[0]), mean_wave, yerr=std_wave, color='blue', linewidth=2, label='Avg. Waveform')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        ul=spike_list[labels==i]\n",
    "        ull=ul\n",
    "        #ull=ul[indices_to_keep]\n",
    "        final_data.append(ull)\n",
    "        final_firing.append(firings)\n",
    "        plt.subplot(3, 1, i + 1)\n",
    "        plt.hist(np.diff(ull), bins=100, density=True, alpha=0.5, color='blue', edgecolor='black')\n",
    "        plt.title(f'ISI: Cluster {i}, \\n firing rate: {format(len(final_data[i])*10000/len(data), \".2f\")} Hz')\n",
    "        plt.show()\n",
    "        \n",
    "    return final_data, final_firing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

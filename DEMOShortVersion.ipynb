{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applicato a 6 file totali (invece di 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e716d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_neurons=[]\n",
    "from NewLibraryENGcopia import *\n",
    "import glob\n",
    "#final=[]\n",
    "list_dir=glob.glob(\"/Users/Gaia_1/Desktop/allh5filestutto/*\")\n",
    "output_path='/Users/Gaia_1/Desktop/tesi/Data after SS'\n",
    "for file in tqdm(list_dir):\n",
    "    target=1\n",
    "    stim=0\n",
    "    if 'health' in file:\n",
    "        target=0\n",
    "    if 'stimulation' in file:\n",
    "        stim=1\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    print(file_name,':','target',target,'stimulation',stim)\n",
    "    neurons=spike_sorting(file,output_path)\n",
    "    #df=poiproc(neurons,target,stim)\n",
    "    #final.append(df)\n",
    "    final_neurons.append(neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a680fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from NewLibraryENGcopia import *\n",
    "import glob\n",
    "final=[]\n",
    "output_path='/Users/Gaia_1/Desktop/tesi/Data after SS'\n",
    "list_dir=glob.glob(output_path+\"/*\")\n",
    "for file in tqdm(list_dir):\n",
    "    target=1\n",
    "    stim=0\n",
    "    if 'health' in file:\n",
    "        target=0\n",
    "    if 'stimulation' in file:\n",
    "        stim=1\n",
    "    list_neurons = np.genfromtxt(file, delimiter=',')\n",
    "    df=poiproc(list_neurons,target,stim)\n",
    "    final.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=this_poiproc(neurons,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c70fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NewLibraryENGcopia import *\n",
    "import glob\n",
    "final=[]\n",
    "list_dir=glob.glob(\"/Users/Gaia_1/Desktop/tesi/Data after SS/*\")\n",
    "for file in list_dir:\n",
    "    target=1\n",
    "    stim=0\n",
    "    if 'health' in file:\n",
    "        target=0\n",
    "    if 'stimulation' in file:\n",
    "        stim=1\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    print(file_name,':','target',target,'stimulation',stim)\n",
    "    neurons = np.genfromtxt(file, delimiter=',')\n",
    "    df=poiproc(neurons,target,stim)\n",
    "    final.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NewLibraryENGcopia import *\n",
    "\n",
    "#POPULATION 1 (HEALTHY):\n",
    "name_data_st = '2019-01-23T16-06-32Pop1 healthy cortical .h5'\n",
    "name_data_BL = '2019-01-23T11-19-05PoP1 healthy cortical .h5'\n",
    "name_data_24 = '2019-01-24T16-00-33Pop1 24hour after.h5'\n",
    "complete_string='/Users/Gaia_1/Desktop/allh5files/healthy/healthy_baseline/'+name_data_BL\n",
    "#complete_string='/Users/Gaia_1/Desktop/allh5files/healthy/healthy_stimulation/'+name_data_st\n",
    "#complete_string='/Users/Gaia_1/Desktop/allh5files/healthy/healthy 24hrs later/'+name_data_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226546a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from NewLibraryENGcopia import *\n",
    "import glob\n",
    "final=[]\n",
    "list_dir=glob.glob(\"/Users/Gaia_1/Desktop/allh5filestutto/*\")\n",
    "for files in list_dir:\n",
    "    print(files)\n",
    "file=list_dir[1]\n",
    "print('chosen:',file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da0d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "target=1\n",
    "stim=0\n",
    "#healthy and LKRR2\n",
    "if 'health' in file:\n",
    "    target=0\n",
    "if 'stimulation' in file:\n",
    "    stim=1\n",
    "#all files\n",
    "file_name = file.split(\"/\")[-1]\n",
    "print(file_name,':','target',target,'stimulation',stim)\n",
    "neurons=spike_sorting(file_name,file,1,4,1.5)\n",
    "df=poiproc(neurons,target,stim)\n",
    "final.append(df)\n",
    "concatenated_dataframes = [elem.T for elem in tqdm(final)]\n",
    "final_result = pd.concat(concatenated_dataframes, axis=0)\n",
    "file_path = '/Users/Gaia_1/Desktop/tesi/Data after PP/'+file_name+'.csv'\n",
    "final_result.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e657f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "target=1\n",
    "stim=0\n",
    "#healthy and LKRR2\n",
    "if 'health' in file:\n",
    "    target=0\n",
    "if 'stimulation' in file:\n",
    "    stim=1\n",
    "#all files\n",
    "file_name = file.split(\"/\")[-1]\n",
    "print(file_name,':','target',target,'stimulation',stim)\n",
    "neurons=spike_sorting(file_name,file,1,4,1.5)\n",
    "df=poiproc(neurons,target,stim)\n",
    "final.append(df)\n",
    "concatenated_dataframes = [elem.T for elem in tqdm(final)]\n",
    "final_result = pd.concat(concatenated_dataframes, axis=0)\n",
    "file_path = '/Users/Gaia_1/Desktop/tesi/Data after PP/'+file_name+'.csv'\n",
    "final_result.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c13a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neurons[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610be6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "df=this_poiproc(neurons[9:],target,stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_poiproc(neurons,target,stim):\n",
    "    from scipy.stats import ks_2samp\n",
    "    dataframe = pd.DataFrame()\n",
    "    counter=0\n",
    "    list_neurons = neurons\n",
    "    counter=0\n",
    "    print('Original number of neurons: ',len(list_neurons))\n",
    "    for neuron in tqdm(list_neurons):\n",
    "        neuron=neuron[neuron>0*10000]\n",
    "        #neuron=neuron[neuron>100*10000]\n",
    "        neuron=neuron[neuron<200*10000]\n",
    "        print('  Neuron with ',neuron.shape[0],'spikes')\n",
    "        if neuron.shape[0]>1000:\n",
    "            \n",
    "            counter+=1\n",
    "        else:\n",
    "            print('    Excluded neuron with n spikes = ',neuron.shape[0])\n",
    "            continue\n",
    "        \n",
    "        ISI_healthy = np.diff(neuron)/10000\n",
    "        map_estimate,trace,ppc_trace = this_Bayesian_mixture_model(ISI_healthy)\n",
    "        map_estimate['Target']=target\n",
    "        map_estimate['Stimulation']=stim\n",
    "        df = pd.DataFrame.from_dict(map_estimate,orient='index')\n",
    "        dataframe = pd.concat([dataframe,df],axis = 1)\n",
    "        lista_samples=[]\n",
    "        for i in list(ppc_trace['posterior_predictive']['like'].values):\n",
    "            lista_samples.extend(i)\n",
    "        #ks_2samp(lista_samples,ISI_healthy,mode = 'asymp')\n",
    "        #print('ks: ',ks_2amp)\n",
    "        ks_statistic, p_value = ks_2samp(np.concatenate(lista_samples), ISI_healthy, mode='asymp')\n",
    "        if p_value>=0.69:\n",
    "            print('p value high enough')\n",
    "            az.plot_trace(trace)\n",
    "        print('ks_statistic:', ks_statistic,'p_value:', p_value)\n",
    "        \n",
    "    print('Final number of neurons: ',counter)\n",
    "    print('Target = ',target)\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6011299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190dfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_Bayesian_mixture_model(ISI_data):\n",
    "    with pm.Model() as model:\n",
    "        ##### WALD DISTRIBUTION (INVERSE GAUSSIAN)\n",
    "        mu1 = pm.Uniform('mu1',lower=0.01,upper=0.1)\n",
    "        lam1 = pm.Uniform('lam1',lower=0.01,upper=0.04)\n",
    "        obs1 = pm.Wald.dist(mu=mu1,lam=lam1)\n",
    "\n",
    "\n",
    "        mu2 = pm.Uniform('mu2',lower=0,upper=0.2)\n",
    "        sigma2 = pm.Uniform('sigma2',lower=0.0001,upper=0.5)\n",
    "        obs2 = pm.TruncatedNormal.dist(mu=mu2, sigma=sigma2, lower=0.0)\n",
    "\n",
    "        mu3 = pm.Uniform('mu3',lower=0.1,upper=0.6)\n",
    "        sigma3 = pm.Uniform('sigma3',lower=0.0001,upper=0.5)\n",
    "        obs3 = pm.TruncatedNormal.dist(mu=mu3, sigma=sigma3, lower=0.0)\n",
    "\n",
    "        w = pm.Dirichlet('w', a=np.array([1., .4, .4]))\n",
    "        like = pm.Mixture('like', w=w, comp_dists = [obs1, obs2, obs3], observed=ISI_data)\n",
    "\n",
    "        step = pm.NUTS(target_accept=0.95)\n",
    "        trace = pm.sample(step=step,draws=1000,chains=4,tune=1000,cores=4)\n",
    "        ppc_trace = pm.sample_posterior_predictive(trace,model=model)\n",
    "        #import matplotlib.pyplot as plt\n",
    "        #pm.traceplot(trace)\n",
    "        #az.plot_trace(trace)\n",
    "        plt.show()\n",
    "\n",
    "    map_estimate = pm.find_MAP(model=model)\n",
    "    \n",
    "    del map_estimate['w_simplex__']\n",
    "    del map_estimate['mu1_interval__']\n",
    "    del map_estimate['lam1_interval__']\n",
    "    del map_estimate['mu2_interval__']\n",
    "    del map_estimate['sigma2_interval__']\n",
    "    del map_estimate['mu3_interval__']\n",
    "    del map_estimate['sigma3_interval__']\n",
    "    \n",
    "    map_estimate['w1'] = map_estimate['w'][0]\n",
    "    map_estimate['w2'] = map_estimate['w'][1]\n",
    "    map_estimate['w3'] = map_estimate['w'][2]\n",
    "\n",
    "    del map_estimate['w']\n",
    "\n",
    "\n",
    "    return map_estimate,trace,ppc_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb6e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d5e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concatenated_dataframes = [elem.T for elem in tqdm(final)]\n",
    "final_result = pd.concat(concatenated_dataframes, axis=0)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_name)\n",
    "file_path = '/Users/Gaia_1/Desktop/tesi/Data after PP/'+file_name+'.csv'\n",
    "final_result.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73787943",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data after PP/'+file_name+'.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12745f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b279a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset=final_result\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aba3f9",
   "metadata": {},
   "source": [
    "dataset = dataset.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "X = dataset.drop(['Target','Stimulation'],axis=1)\n",
    "y= dataset['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a490b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321dc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5,svd_solver='full')\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(pca.fit(X_train).explained_variance_ratio_,sum(pca.fit(X_train).explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bf22a",
   "metadata": {},
   "source": [
    "# Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth':np.linspace(2,10,dtype='int'),'criterion':['gini','entropy']}\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=12)\n",
    "\n",
    "gs = GridSearchCV(clf, params, scoring='roc_auc',cv=cv,n_jobs=-1)\n",
    "\n",
    "gs=gs.fit(X_train,y_train)\n",
    "\n",
    "gs_pca = GridSearchCV(clf, params, scoring='roc_auc',cv=cv,n_jobs=-1)\n",
    "gs_pca = gs_pca.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found :\",gs.best_params_,'\\n')\n",
    "print(\"Classification report on Test set\\n\")\n",
    "\n",
    "y_true, y_pred = y_test, gs.predict(X_test)\n",
    "\n",
    "accuracy = round(accuracy_score(y_true, y_pred),3)\n",
    "recall = round(recall_score(y_true, y_pred),3)\n",
    "precision = round(precision_score(y_true, y_pred),3)\n",
    "roc_auc = round(roc_auc_score(y_true, y_pred),3)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Recall: ',recall)\n",
    "print('Precision: ',precision)\n",
    "print('ROC AUC: ',roc_auc)\n",
    "print('\\n\\n Result with PCA\\n')\n",
    "print(\"Best parameters set found :\",gs_pca.best_params_,'\\n')\n",
    "print(\"Classification report on Test set\\n\")\n",
    "\n",
    "y_true, y_pred = y_test, gs_pca.predict(X_test_pca)\n",
    "\n",
    "accuracy = round(accuracy_score(y_true, y_pred),3)\n",
    "recall = round(recall_score(y_true, y_pred),3)\n",
    "precision = round(precision_score(y_true, y_pred),3)\n",
    "roc_auc = round(roc_auc_score(y_true, y_pred),3)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Recall: ',recall)\n",
    "print('Precision: ',precision)\n",
    "print('ROC AUC: ',roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af6ff4",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':np.linspace(10,150,dtype='int',num=10),'max_depth':np.linspace(2,5,dtype='int'),'criterion':['gini','entropy']}\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=-1,random_state=12)\n",
    "\n",
    "gs = GridSearchCV(clf, params, scoring='roc_auc',cv=cv,verbose=10,n_jobs=-1)\n",
    "\n",
    "gs=gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found :\",gs.best_params_,'\\n')\n",
    "print(\"Classification report on Test set\\n\")\n",
    "\n",
    "y_true, y_pred = y_test, gs.predict(X_test)\n",
    "\n",
    "accuracy = round(accuracy_score(y_true, y_pred),3)\n",
    "recall = round(recall_score(y_true, y_pred),3)\n",
    "precision = round(precision_score(y_true, y_pred),3)\n",
    "roc_auc = round(roc_auc_score(y_true, y_pred),3)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Recall: ',recall)\n",
    "print('Precision: ',precision)\n",
    "print('ROC AUC: ',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca0e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804d6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3375bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_poiproc(neurons,target,stim):\n",
    "    from scipy.stats import ks_2samp\n",
    "    dataframe = pd.DataFrame()\n",
    "    #counter_net=1\n",
    "    counter=0\n",
    "    #for net in list_dir_ok:\n",
    "    #print(counter_net,') ',net)\n",
    "    #counter_net+=1\n",
    "    list_neurons = neurons #np.genfromtxt(net, delimiter=',')\n",
    "    counter=0\n",
    "    print('Original number of neurons: ',len(list_neurons))\n",
    "    for neuron in tqdm(list_neurons):\n",
    "        neuron=neuron[neuron>0*10000]\n",
    "        neuron=neuron[neuron<200*10000]\n",
    "        print('  Neuron with ',neuron.shape[0],'spikes')\n",
    "        if neuron.shape[0]>1000:\n",
    "            \n",
    "            counter+=1\n",
    "        else:\n",
    "            print('    Excluded neuron with n spikes = ',neuron.shape[0])\n",
    "            continue\n",
    "        \n",
    "        ISI_healthy = np.diff(neuron)/10000\n",
    "        map_estimate = Bayesian_mixture_model(ISI_healthy)\n",
    "        map_estimate['Target']=int(target)\n",
    "        map_estimate['Stimulation']=int(stim)\n",
    "        df = pd.DataFrame.from_dict(map_estimate,orient='index')\n",
    "        dataframe = pd.concat([dataframe,df],axis = 1)\n",
    "    print('Final number of neurons: ',counter)\n",
    "    print('Target = ',target)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "    #ks_2samp(lista_samples,ISI_healthy,mode = 'asymp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143613d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_spike_sorting(name_data,complete_string,threshold,clustering,coeff,c1):\n",
    "    #file reading:\n",
    "    data = h5py.File(complete_string,'r')\n",
    "    data_readings = data['Data']['Recording_0']['AnalogStream']['Stream_0']['ChannelData'][()]\n",
    "    info = data['Data']['Recording_0']['AnalogStream']['Stream_0']['InfoChannel'][()]\n",
    "    info_table = pd.DataFrame(info, columns = list(info.dtype.fields.keys()))\n",
    "    labels = info_table['Label']\n",
    "    readings = pd.DataFrame(data = data_readings.transpose(), columns = labels)\n",
    "    fs = 10000 #Sampling Frequency\n",
    "    print('data shape: ',readings.shape)\n",
    "    prova=readings.drop([b'Ref'],axis=1)\n",
    "    #prova=prova.iloc[inizio:fine, :10]\n",
    "    #prova=prova.iloc[:, :10]\n",
    "    ref=readings[b'Ref']\n",
    "    #ref=ref[inizio:fine]\n",
    "    #filtering:\n",
    "    prova_rows = range(prova.shape[0])\n",
    "    filt_prova = pd.DataFrame(data = 0, columns=prova.columns, index=prova_rows, dtype = \"float32\")\n",
    "    lowcut = 300\n",
    "    highcut = 3000\n",
    "    fs=10000\n",
    "    order=8\n",
    "    b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "    filt_ref=filtfilt(b,a,ref)\n",
    "    for x in tqdm(range(prova.shape[1])):\n",
    "        filt_prova.values[:,x] = scipy.signal.filtfilt(b, a, prova.values[:,x])\n",
    "    for electrode in prova.columns:\n",
    "        filt_prova[electrode] = filt_prova[electrode] - filt_ref\n",
    "    prova=filt_prova\n",
    "    #detection:\n",
    "    if threshold==0:\n",
    "        threshold=[]\n",
    "        for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "            threshold.append(coeff*(scipy.stats.median_abs_deviation(prova[electrode].values,scale='normal')))            \n",
    "    all_ind=[]\n",
    "    for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "        channel=prova[electrode]\n",
    "        thresh=threshold[i]\n",
    "        ind=find_all_spikes(channel,thresh)\n",
    "        all_ind.append(ind)\n",
    "    #spike extraction:\n",
    "    cut_outs=[]\n",
    "    all_new=[]\n",
    "    for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "        ind=all_ind[i]\n",
    "        channel=prova[electrode]\n",
    "        cut_outs1,all_new1=cut_all(ind,channel,c1)\n",
    "        cut_outs.append(cut_outs1)\n",
    "        all_new.append(all_new1)    \n",
    "    # Clustering:\n",
    "    final_data=[]\n",
    "    if clustering=='kmeans':\n",
    "            for channel in (tqdm(range(len(cut_outs)))):\n",
    "                #channel_clusters1=comparative_clus(cut_outs[channel],all_new[channel],prova.iloc[:,channel])\n",
    "                channel_clusters1=clus(cut_outs[channel],all_new[channel],prova.iloc[:,channel])\n",
    "                final_data.append(channel_clusters1)\n",
    "    elif clustering=='dbscan':\n",
    "            for channel in (tqdm(range(len(cut_outs)))):\n",
    "                eps=int(scipy.stats.median_abs_deviation(prova.iloc[:,channel])/2)\n",
    "                channel_clusters1=dbscan_clustering(cut_outs[channel],all_new[channel],prova.iloc[:,channel],eps)\n",
    "                final_data.append(channel_clusters1)\n",
    "    neurons=[]\n",
    "    for channel in final_data:\n",
    "        for neuron in channel:\n",
    "            neurons.append(neuron)\n",
    "    print(len(neurons),' neurons detected and sorted')\n",
    "    adj_neur=[]\n",
    "    counter = 0\n",
    "    max_len=0\n",
    "    for neuron in neurons:\n",
    "        print('counter: ',counter,neuron.shape[0])\n",
    "        if neuron.shape[0]>max_len:\n",
    "            max_len=neuron.shape[0]\n",
    "        counter+=1\n",
    "    for neuron in neurons:\n",
    "        if neuron.shape[0]<max_len and neuron.shape[0]>=1000:\n",
    "            diff = max_len-neuron.shape[0]\n",
    "            adj_neur.append(np.concatenate((neuron,np.zeros([diff]))))\n",
    "    save_data = 'After'+name_data+'.txt'\n",
    "    np.savetxt(\"/Users/Gaia_1/Desktop/tesi/Data after SS prova/%s.txt\" % save_data,adj_neur, delimiter=', ', fmt='%12.8f')\n",
    "    print(save_data)\n",
    "    return neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2f63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb252cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two recordings (BL and stimulation) from the same population\n",
    "# load both datasets, pre-process separately, detect spikes, \n",
    "# cluster separately, cluster average spike from BL with average spike from stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NewLibraryENG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e791ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data_BL = '2019-01-23T11-19-05PoP1 healthy cortical .h5'\n",
    "name_data_st = '2019-01-23T16-06-32Pop1 healthy cortical .h5'\n",
    "name_data_24 = '2019-01-24T16-00-33Pop1 24hour after.h5'\n",
    "\n",
    "complete_string_BL='/Users/Gaia_1/Desktop/allh5files/healthy/healthy_baseline/'+name_data_BL\n",
    "complete_string_st='/Users/Gaia_1/Desktop/allh5files/healthy/healthy_stimulation/'+name_data_st\n",
    "complete_string_24='/Users/Gaia_1/Desktop/allh5files/healthy/healthy 24hrs later/'+name_data_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b798fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "data = h5py.File(complete_string_BL,'r')\n",
    "\n",
    "data_readings = data['Data']['Recording_0']['AnalogStream']['Stream_0']['ChannelData'][()]\n",
    "info = data['Data']['Recording_0']['AnalogStream']['Stream_0']['InfoChannel'][()]\n",
    "info_table = pd.DataFrame(info, columns = list(info.dtype.fields.keys()))\n",
    "labels = info_table['Label']\n",
    "readings_BL = pd.DataFrame(data = data_readings.transpose(), columns = labels)\n",
    "fs = 10000 #Sampling Frequency\n",
    "print(readings_BL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimulation\n",
    "data = h5py.File(complete_string_st,'r')\n",
    "\n",
    "data_readings = data['Data']['Recording_0']['AnalogStream']['Stream_0']['ChannelData'][()]\n",
    "info = data['Data']['Recording_0']['AnalogStream']['Stream_0']['InfoChannel'][()]\n",
    "info_table = pd.DataFrame(info, columns = list(info.dtype.fields.keys()))\n",
    "labels = info_table['Label']\n",
    "readings_st = pd.DataFrame(data = data_readings.transpose(), columns = labels)\n",
    "fs = 10000 #Sampling Frequency\n",
    "print(readings_st.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#24hrs\n",
    "data = h5py.File(complete_string_24,'r')\n",
    "\n",
    "data_readings = data['Data']['Recording_0']['AnalogStream']['Stream_0']['ChannelData'][()]\n",
    "info = data['Data']['Recording_0']['AnalogStream']['Stream_0']['InfoChannel'][()]\n",
    "info_table = pd.DataFrame(info, columns = list(info.dtype.fields.keys()))\n",
    "labels = info_table['Label']\n",
    "readings_24 = pd.DataFrame(data = data_readings.transpose(), columns = labels)\n",
    "fs = 10000 #Sampling Frequency\n",
    "print(readings_24.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ac201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "readings_BL.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "inizio=0\n",
    "len_data=1920000 #192 secondi\n",
    "#len_data=len(readings)\n",
    "ref_BL=readings_BL[b'Ref']\n",
    "ref_st=readings_st[b'Ref']\n",
    "ref_24=readings_24[b'Ref']\n",
    "prova_BL=readings_BL.drop([b'Ref'],axis=1)\n",
    "prova_st=readings_st.drop([b'Ref'],axis=1)\n",
    "prova_24=readings_24.drop([b'Ref'],axis=1)\n",
    "prova_BL=prova_BL.iloc[inizio:len_data, 10:]\n",
    "prova_BL=prova_BL[b'53']\n",
    "prova_st=prova_st.iloc[inizio:len_data, 10:]\n",
    "prova_st=prova_st[b'53']\n",
    "prova_24=prova_24.iloc[inizio:len_data, 10:]\n",
    "prova_24=prova_24[b'53']\n",
    "ref_BL=ref_BL[inizio:len_data]\n",
    "ref_st=ref_st[inizio:len_data]\n",
    "ref_24=ref_24[inizio:len_data]\n",
    "\n",
    "print(prova_BL.shape,ref_BL.shape,prova_st.shape,ref_st.shape,prova_24.shape,ref_24.shape)\n",
    "prova_BL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcut = 300\n",
    "highcut = 3000\n",
    "fs=10000\n",
    "order=8\n",
    "b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "filt_ref=filtfilt(b,a,ref_BL)\n",
    "filt_prova=filtfilt(b,a,prova_BL)\n",
    "f_prova = filt_prova - filt_ref\n",
    "prova_BL=f_prova\n",
    "\n",
    "filt_ref=filtfilt(b,a,ref_st)\n",
    "filt_prova=filtfilt(b,a,prova_st)\n",
    "f_prova = filt_prova - filt_ref\n",
    "prova_st=f_prova\n",
    "\n",
    "filt_ref=filtfilt(b,a,ref_24)\n",
    "filt_prova=filtfilt(b,a,prova_24)\n",
    "f_prova = filt_prova - filt_ref\n",
    "prova_24=f_prova\n",
    "\n",
    "print(prova_BL.shape,prova_st.shape,prova_24.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967a5d2",
   "metadata": {},
   "source": [
    "#BL\n",
    "prova=prova_BL\n",
    "\n",
    "prova_rows = range(prova.shape[0])\n",
    "filt_prova = pd.DataFrame(data = 0, columns=prova.columns, index=prova_rows, dtype = \"float32\")\n",
    "lowcut = 300\n",
    "highcut = 3000\n",
    "fs=10000\n",
    "order=8\n",
    "b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "filt_ref=filtfilt(b,a,ref_BL)\n",
    "#ref_df = pd.DataFrame({b'Ref': filt_ref})\n",
    "for x in tqdm(range(prova.shape[1])):\n",
    "    filt_prova.values[:,x] = scipy.signal.filtfilt(b, a, prova.values[:,x])\n",
    "for electrode in prova.columns:\n",
    "    filt_prova[electrode] = filt_prova[electrode] - filt_ref\n",
    "#filt_prova = pd.concat([filt_prova, ref_df], axis=1)\n",
    "prova_BL=filt_prova\n",
    "prova_BL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8dc3b",
   "metadata": {},
   "source": [
    "#stim\n",
    "prova=prova_st\n",
    "\n",
    "prova_rows = range(prova.shape[0])\n",
    "filt_prova = pd.DataFrame(data = 0, columns=prova.columns, index=prova_rows, dtype = \"float32\")\n",
    "lowcut = 300\n",
    "highcut = 3000\n",
    "fs=10000\n",
    "order=8\n",
    "b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "filt_ref=filtfilt(b,a,ref_st)\n",
    "#ref_df = pd.DataFrame({b'Ref': filt_ref})\n",
    "for x in tqdm(range(prova.shape[1])):\n",
    "    filt_prova.values[:,x] = scipy.signal.filtfilt(b, a, prova.values[:,x])\n",
    "for electrode in prova.columns:\n",
    "    filt_prova[electrode] = filt_prova[electrode] - filt_ref\n",
    "#filt_prova = pd.concat([filt_prova, ref_df], axis=1)\n",
    "prova_st=filt_prova\n",
    "prova_st.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce846b51",
   "metadata": {},
   "source": [
    "prova_st.index+=1920000\n",
    "prova_st.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92169e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mer=pd.concat([prova_BL,prova_st],axis=0)\n",
    "mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac63871",
   "metadata": {},
   "outputs": [],
   "source": [
    "mer.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4280ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_index=1920000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge=np.concatenate((prova_BL, prova_st), axis=0)\n",
    "len(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_BL, neg_BL=find_all_spikes(prova_BL)\n",
    "pos_st, neg_st=find_all_spikes(prova_st)\n",
    "pos_24, neg_24=find_all_spikes(prova_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b249b",
   "metadata": {},
   "source": [
    "prova=prova_BL\n",
    "pos_ind=[]\n",
    "neg_ind=[]\n",
    "for electrode in tqdm(prova.columns):\n",
    "    channel=prova[electrode]\n",
    "    pos, neg=find_all_spikes(channel)\n",
    "    pos_ind_BL.append(pos)\n",
    "    neg_ind_BL.append(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07c24b",
   "metadata": {},
   "source": [
    "prova=prova_st\n",
    "pos_ind=[]\n",
    "neg_ind=[]\n",
    "for electrode in tqdm(prova.columns):\n",
    "    channel=prova[electrode]\n",
    "    pos, neg=find_all_spikes(channel)\n",
    "    pos_ind_st.append(pos)\n",
    "    neg_ind_st.append(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf688bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cut_BL,n_pos_BL, neg_cut_BL,n_neg_BL = cut(pos_BL,neg_BL,prova_BL)\n",
    "pos_cut_st,n_pos_st, neg_cut_st,n_neg_st = cut(pos_st,neg_st,prova_st)\n",
    "pos_cut_24,n_pos_24, neg_cut_24,n_neg_24 = cut(pos_24,neg_24,prova_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cut_BL=mask_cuts(pos_cut_BL)\n",
    "neg_cut_BL= mask_cuts(neg_cut_BL)\n",
    "\n",
    "pos_cut_st=mask_cuts(pos_cut_st)\n",
    "neg_cut_st= mask_cuts(neg_cut_st)\n",
    "\n",
    "pos_cut_24=mask_cuts(pos_cut_24)\n",
    "neg_cut_24= mask_cuts(neg_cut_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823aedf",
   "metadata": {},
   "source": [
    "pos_cut=[]\n",
    "neg_cut=[]\n",
    "n_pos=[]\n",
    "n_neg=[]\n",
    "\n",
    "for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "    pos=pos_ind[i]\n",
    "    neg=neg_ind[i]\n",
    "    channel=prova[electrode]\n",
    "    pos_cut1,n_pos1, neg_cut1,n_neg1 = cut(pos,neg,channel)\n",
    "    pos_cut.append(pos_cut1)\n",
    "    neg_cut.append(neg_cut1)\n",
    "    n_pos.append(n_pos1)\n",
    "    n_neg.append(n_neg1)\n",
    "#savedp = copy.deepcopy(pos_cut)\n",
    "#savedn = copy.deepcopy(neg_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca4ab8",
   "metadata": {},
   "source": [
    "for i in (tqdm(range(len(pos_cut)))):\n",
    "    pos_cut[i]=mask_cuts(pos_cut[i])\n",
    "    neg_cut[i]= mask_cuts(neg_cut[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d3b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6ce53fa",
   "metadata": {},
   "source": [
    "final_data_pos=[]\n",
    "for channel in (tqdm(range(len(pos_cut)))):\n",
    "    channel_clusters=clus(pos_cut[channel],'kmeans',n_pos[channel],prova.iloc[:,channel])\n",
    "    final_data_pos.append(channel_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0e45f",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_pos_BL= clus(pos_cut_BL,'kmeans',n_pos_BL,prova_BL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6cc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25259c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577509fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83633b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spike_list=n_pos\n",
    "spike_list\n",
    "print(switch_index,max(spike_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629eb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_list=np.array(spike_list)\n",
    "spike_list1=[]\n",
    "for spike_ind in tqdm(spike_list):\n",
    "    if spike_ind<=switch_index:\n",
    "        spike_list1.append(spike_ind)\n",
    "spike_list1=np.array(spike_list1)\n",
    "end_spikelist1=len(spike_list1)-1\n",
    "spike_list2=spike_list[end_spikelist1+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spike_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cut[:end_spikelist1+1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ec8a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def clus(cut,clustering,spike_list,data):\n",
    "cut=pos_cut\n",
    "clustering='kmeans'\n",
    "spike_list=n_pos\n",
    "data=merge\n",
    "#switch_index=\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import kurtosis\n",
    "import skfuzzy as fuzz\n",
    "import numpy as np\n",
    "import math\n",
    "n_min=3\n",
    "n_tries=15\n",
    "spike_list=np.array(spike_list)\n",
    "spike_list1=[]\n",
    "for spike_ind in tqdm(spike_list):\n",
    "    if spike_ind<=switch_index:\n",
    "        spike_list1.append(spike_ind)\n",
    "spike_list1=np.array(spike_list1)\n",
    "end_spikelist1=len(spike_list1)-1\n",
    "spike_list2=spike_list[end_spikelist1+1:]\n",
    "\n",
    "len_data1=len(data[:switch_index])\n",
    "len_data2=len(data[switch_index+1:])\n",
    "\n",
    "cut1=pos_cut[:end_spikelist1]\n",
    "cut2=pos_cut[end_spikelist1+1:]\n",
    "\n",
    "scale = StandardScaler()\n",
    "estratti_norm = scale.fit_transform(cut1)\n",
    "print('Total spikes cut1: ', estratti_norm.shape[0])\n",
    "n_comp=10\n",
    "pca = PCA(n_components=n_comp)\n",
    "transformed1 = pca.fit_transform(estratti_norm)\n",
    "\n",
    "estratti_norm = scale.fit_transform(cut2)\n",
    "print('Total spikes cut2: ', estratti_norm.shape[0])\n",
    "n_comp=10\n",
    "pca = PCA(n_components=n_comp)\n",
    "transformed2 = pca.fit_transform(estratti_norm)\n",
    "\n",
    "\n",
    "info1=[]\n",
    "info2=[]\n",
    "#list_score=[]\n",
    "#DB_score=[]\n",
    "best_score=[]\n",
    "if clustering=='kmeans':\n",
    "    for n in range (n_min,n_tries):\n",
    "        model = KMeans(n_clusters=n, n_init='auto', copy_x=True, algorithm='lloyd')\n",
    "        labels1 = model.fit_predict(transformed1)\n",
    "        labels2 = model.fit_predict(transformed2)\n",
    "        if (n != 1):\n",
    "            silhouette_avg1 = silhouette_score(transformed1, labels1)\n",
    "            silhouette_avg2 = silhouette_score(transformed2, labels2)\n",
    "            print(\"For\", n,\"clusters, the silhouette score is:\", format(silhouette_avg1, \".3f\"), format(silhouette_avg2, \".3f\"))\n",
    "            best_score.append(silhouette_avg1+silhouette_avg2)\n",
    "            #del(model)\n",
    "            #del(labels)\n",
    "    top_clusters = (best_score.index(max(best_score)))+n_min\n",
    "    num_clusters=top_clusters\n",
    "    print(\"\\n\\n\\033[1;31;47mBest cluster in the range\", n_min, \"to \", n_tries-1, \": \",top_clusters,\", with a silhouette score of: \",best_score[top_clusters-n_min], \"\\u001b[0m  \\n\\n\")\n",
    "\n",
    "    model = KMeans(n_clusters=top_clusters, n_init='auto', copy_x=True, algorithm='lloyd')\n",
    "    labels1 = model.fit_predict(transformed1)\n",
    "    labels2 = model.fit_predict(transformed2)\n",
    "######################### \n",
    "#adesso abbiamo due clusterizzazioni con lo stesso numero di cluster e bisogna accoppiarli per identificare i neuroni\n",
    "\n",
    "#primo clustering\n",
    "final_data=[]\n",
    "temporary_data=[]\n",
    "labels=labels1\n",
    "cut=cut1\n",
    "unique_labels = np.unique(labels)\n",
    "firings=np.zeros(len(unique_labels))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "\n",
    "# Iterate over unique cluster labels\n",
    "for i, cluster_label in enumerate(unique_labels):\n",
    "    # Extract data points for the current cluster\n",
    "    cluster_data = cut[labels == cluster_label]\n",
    "    firings[i]=len(cluster_data)*10000/len_data\n",
    "\n",
    "    # Plot the individual cluster data\n",
    "    if len(unique_labels)<=2:\n",
    "        size1=len(unique_labels)\n",
    "        size2=1\n",
    "    elif len(unique_labels)<=5:\n",
    "        size1 = math.ceil(len(unique_labels)/2)\n",
    "        size2=size1\n",
    "    elif len(unique_labels)<=8:\n",
    "        size1 = 3\n",
    "        size2=math.ceil(len(unique_labels)/size1)\n",
    "    else:\n",
    "        size1=6\n",
    "        size2=math.ceil(len(unique_labels)/size1)\n",
    "    plt.subplot(size1,size2, i + 1)\n",
    "    plt.plot(cluster_data.transpose(), alpha=0.5)  # Use alpha for transparency\n",
    "    #print(cluster_data)\n",
    "    plt.title(f'Cluster {cluster_label} \\n numerosity: {len(cluster_data)}')\n",
    "    plt.xlabel('Time [ms]')\n",
    "    plt.ylabel('Signal Amplitude')\n",
    "\n",
    "    # Plot the average waveform\n",
    "    mean_wave = np.mean(cluster_data, axis=0)\n",
    "    #info.append(f'mean clus{cluster_label}')\n",
    "    info1.append(mean_wave)\n",
    "    std_wave = np.std(cluster_data, axis=0)\n",
    "    plt.errorbar(range(mean_wave.shape[0]), mean_wave, yerr=std_wave, color='black', linewidth=2, label='Avg. Waveform')\n",
    "    plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "\n",
    "####secondo clustering\n",
    "final_data=[]\n",
    "temporary_data=[]\n",
    "labels=labels2\n",
    "cut=cut2\n",
    "unique_labels = np.unique(labels)\n",
    "firings=np.zeros(len(unique_labels))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "\n",
    "# Iterate over unique cluster labels\n",
    "for i, cluster_label in enumerate(unique_labels):\n",
    "    # Extract data points for the current cluster\n",
    "    cluster_data = cut[labels == cluster_label]\n",
    "    firings[i]=len(cluster_data)*10000/len_data\n",
    "\n",
    "    # Plot the individual cluster data\n",
    "    if len(unique_labels)<=2:\n",
    "        size1=len(unique_labels)\n",
    "        size2=1\n",
    "    elif len(unique_labels)<=5:\n",
    "        size1 = math.ceil(len(unique_labels)/2)\n",
    "        size2=size1\n",
    "    elif len(unique_labels)<=8:\n",
    "        size1 = 3\n",
    "        size2=math.ceil(len(unique_labels)/size1)\n",
    "    else:\n",
    "        size1=6\n",
    "        size2=math.ceil(len(unique_labels)/size1)\n",
    "    plt.subplot(size1,size2, i + 1)\n",
    "    plt.plot(cluster_data.transpose(), alpha=0.5)  # Use alpha for transparency\n",
    "    #print(cluster_data)\n",
    "    plt.title(f'Cluster {cluster_label} \\n numerosity: {len(cluster_data)}')\n",
    "    plt.xlabel('Time [ms]')\n",
    "    plt.ylabel('Signal Amplitude')\n",
    "\n",
    "    # Plot the average waveform\n",
    "    mean_wave = np.mean(cluster_data, axis=0)\n",
    "    #info.append(f'mean clus{cluster_label}')\n",
    "    info1.append(mean_wave)\n",
    "    std_wave = np.std(cluster_data, axis=0)\n",
    "    plt.errorbar(range(mean_wave.shape[0]), mean_wave, yerr=std_wave, color='black', linewidth=2, label='Avg. Waveform')\n",
    "    plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "\n",
    "########### cambiato fino a qua\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(info1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cb5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##qua bisogna creare le coppie\n",
    "\n",
    "\n",
    "model = KMeans(n_clusters=9, n_init='auto', copy_x=True, algorithm='lloyd')\n",
    "labels = model.fit_predict(info1)\n",
    "        \n",
    "final_data=[]\n",
    "temporary_data=[]\n",
    "#labels=labels2\n",
    "cut=info1\n",
    "unique_labels = np.unique(labels)\n",
    "firings=np.zeros(len(unique_labels))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "print(labels,unique_labels)\n",
    "# Iterate over unique cluster labels\n",
    "for i, cluster_label in enumerate(unique_labels):\n",
    "    # Extract data points for the current cluster\n",
    "    cluster_data = cut[labels == cluster_label]\n",
    "    firings[i]=len(cluster_data)*10000/len_data\n",
    "\n",
    "    # Plot the individual cluster data\n",
    "    if len(unique_labels)<=2:\n",
    "        size1=len(unique_labels)\n",
    "        size2=1\n",
    "    elif len(unique_labels)<=5:\n",
    "        size1 = math.ceil(len(unique_labels)/2)\n",
    "        size2=size1\n",
    "    elif len(unique_labels)<=8:\n",
    "        size1 = 3\n",
    "        size2=math.ceil(len(unique_labels)/size1)\n",
    "    else:\n",
    "        size1=6\n",
    "        size2=math.ceil(len(unique_labels)/size1)\n",
    "    plt.subplot(size1,size2, i + 1)\n",
    "    plt.plot(cluster_data.transpose(), alpha=0.5)  # Use alpha for transparency\n",
    "    #print(cluster_data)\n",
    "    plt.title(f'Cluster {cluster_label} \\n numerosity: {len(cluster_data)}')\n",
    "    plt.xlabel('Time [ms]')\n",
    "    plt.ylabel('Signal Amplitude')\n",
    "\n",
    "    # Plot the average waveform\n",
    "    mean_wave = np.mean(cluster_data, axis=0)\n",
    "    #info.append(f'mean clus{cluster_label}')\n",
    "    info1.append(mean_wave)\n",
    "    std_wave = np.std(cluster_data, axis=0)\n",
    "    plt.errorbar(range(mean_wave.shape[0]), mean_wave, yerr=std_wave, color='black', linewidth=2, label='Avg. Waveform')\n",
    "    plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dee09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_firing=np.mean(firings)\n",
    "std_firing=np.std(firings)\n",
    "firing_threshold=mean_firing-std_firing\n",
    "print('firing rate threshold: ',firing_threshold)\n",
    "info.append('firing threshold')\n",
    "info.append(firing_threshold)\n",
    "info.append('mean firing')\n",
    "info.append(mean_firing)\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "for i in range(0,len(unique_labels)):\n",
    "    ul=spike_list[labels==i]\n",
    "    temporary_data.append(ul)\n",
    "    fr=len(temporary_data[i])*10000/len_data\n",
    "    if i != -1 and fr>firing_threshold:\n",
    "        final_data.append(ul)\n",
    "    plt.subplot(size1, size2, i + 1)\n",
    "    plt.hist(np.diff(ul), bins=100, density=True, alpha=0.5, color='blue', edgecolor='black')\n",
    "    plt.title(f'ISI: Cluster {i} \\n numerosity: {len(temporary_data[i])}, \\n firing rate: {format(len(temporary_data[i])*10000/len_data, \".3f\")}')\n",
    "plt.subplots_adjust(hspace=2.5)\n",
    "plt.show()\n",
    "del(unique_labels)\n",
    "return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a8833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03da45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3fd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0563c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be2fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_data_pos= clus(pos_cut,'kmeans',n_pos,merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_neg=[]\n",
    "\n",
    "for channel in (tqdm(range(len(neg_cut)))):\n",
    "    channel_clusters=clus(neg_cut[channel],'kmeans',n_neg[channel],prova.iloc[:,channel])\n",
    "    final_data_neg.append(channel_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c5a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f3f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

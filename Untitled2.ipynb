{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfcc3c9",
   "metadata": {},
   "source": [
    "# Spike sorting with RMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cff87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_neurons=[]\n",
    "final_firing=[]\n",
    "from NewLibraryENGcopia import *\n",
    "import glob\n",
    "#final=[]\n",
    "list_dir=glob.glob(\"/Users/Gaia_1/Desktop/allh5filestutto/*.h5\")\n",
    "output_path='/Users/Gaia_1/Desktop/tesi/Data after SS RMM'\n",
    "for file in tqdm(list_dir):\n",
    "    target=1\n",
    "    stim=0\n",
    "    if 'health' in file:\n",
    "        target=0\n",
    "    if 'after' in file:\n",
    "        stim=1\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    print(file_name,':','target',target,'stimulation',stim)\n",
    "    neurons,firing=this_spike_sorting(file,output_path)\n",
    "    final_neurons.append(neurons)\n",
    "    final_firing.append(firing)\n",
    "    print(firing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e35bd",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_spike_sorting(input_path,output_path):\n",
    "    name_data = input_path.split(\"/\")[-1]\n",
    "    #file reading:\n",
    "    print('File Reading...')\n",
    "    data = h5py.File(input_path,'r')\n",
    "    data_readings = data['Data']['Recording_0']['AnalogStream']['Stream_0']['ChannelData'][()]\n",
    "    info = data['Data']['Recording_0']['AnalogStream']['Stream_0']['InfoChannel'][()]\n",
    "    info_table = pd.DataFrame(info, columns = list(info.dtype.fields.keys()))\n",
    "    labels = info_table['Label']\n",
    "    readings = pd.DataFrame(data = data_readings.transpose(), columns = labels)\n",
    "    fs = 10000 #Sampling Frequency\n",
    "    print('data shape: ',readings.shape)\n",
    "    prova=readings.drop([b'Ref'],axis=1)\n",
    "    #prova=prova.iloc[0:750500, :]\n",
    "    #prova=prova.iloc[:, :15]\n",
    "    ref=readings[b'Ref']\n",
    "    #ref=ref[0:750500]\n",
    "    freqs, spectrogram = signal.welch(readings[b'Ref'].values, fs=10000, nfft=1024)\n",
    "    noise_freq = freqs[spectrogram.argmax()]\n",
    "    Q = 30\n",
    "    b, a = scipy.signal.iirnotch(noise_freq, Q, fs)\n",
    "    Q = 60\n",
    "    b_2, a_2 = scipy.signal.iirnotch(2*noise_freq, Q, fs)\n",
    "    channel = readings[b'Ref'].values\n",
    "    pre_filtered_ref = scipy.signal.filtfilt(b, a, channel)\n",
    "    pre_filtered_ref = scipy.signal.filtfilt(b_2, a_2, pre_filtered_ref) \n",
    "    ref=pre_filtered_ref\n",
    "\n",
    "    #filtering:\n",
    "    prova_rows = range(prova.shape[0])\n",
    "    filt_prova = pd.DataFrame(data = 0, columns=prova.columns, index=prova_rows, dtype = \"float32\")\n",
    "    lowcut = 300\n",
    "    highcut = 3000\n",
    "    fs=10000\n",
    "    order=8\n",
    "    b,a=butter_bandpass(lowcut,highcut,fs,order=order)\n",
    "    filt_ref=filtfilt(b,a,ref)\n",
    "    print('Data Filtering:')\n",
    "    for x in tqdm(range(prova.shape[1])):\n",
    "        filt_prova.values[:,x] = scipy.signal.filtfilt(b, a, prova.values[:,x])\n",
    "    for electrode in prova.columns:\n",
    "        filt_prova[electrode] = filt_prova[electrode] - filt_ref\n",
    "    prova=filt_prova\n",
    "    #detection:\n",
    "    all_ind=[]\n",
    "    print('Spike Detection: ')\n",
    "    for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "        channel=prova[electrode]\n",
    "        #ind=windowed_spike_detection(channel)\n",
    "        ind=this_spike_detection(channel)\n",
    "        all_ind.append(ind)\n",
    "    #spike extraction:\n",
    "    cut_outs=[]\n",
    "    all_new=[]\n",
    "    print('Spike extraction: ')\n",
    "    for i,electrode in enumerate(tqdm(prova.columns)):\n",
    "        ind=all_ind[i]\n",
    "        channel=prova[electrode]\n",
    "        cut_outs1,all_new1=cut_all(ind,channel)\n",
    "        cut_outs.append(cut_outs1)\n",
    "        all_new.append(all_new1)    \n",
    "    # Clustering:\n",
    "    final_data=[]\n",
    "    final_firing=[]\n",
    "    final_firing.append(name_data)\n",
    "    print('Clustering: ')\n",
    "    for channel in (tqdm(range(len(cut_outs)))):\n",
    "        channel_clusters1,final_firing1=clus(cut_outs[channel],all_new[channel],prova.iloc[:,channel])\n",
    "        final_data.append(channel_clusters1)\n",
    "        final_firing.append(final_firing1)\n",
    "    neurons=[]\n",
    "    for channel in final_data:\n",
    "        for neuron in channel:\n",
    "            neurons.append(neuron)\n",
    "    print(len(neurons),' neurons detected and sorted')\n",
    "    adj_neur=[]\n",
    "    counter = 0\n",
    "    max_len=0\n",
    "    for neuron in neurons:\n",
    "        print('counter: ',counter,neuron.shape[0])\n",
    "        if neuron.shape[0]>max_len:\n",
    "            max_len=neuron.shape[0]\n",
    "        counter+=1\n",
    "    for neuron in neurons:\n",
    "        if neuron.shape[0]<max_len:\n",
    "            diff = max_len-neuron.shape[0]\n",
    "            adj_neur.append(np.concatenate((neuron,np.zeros([diff]))))\n",
    "    save_data = 'After'+name_data+'.txt'\n",
    "    #np.savetxt(\"/Users/Gaia_1/Desktop/tesi/Data after SS/%s.txt\" % save_data,adj_neur, delimiter=', ', fmt='%12.8f')\n",
    "    np.savetxt(f\"{output_path}/{save_data}.txt\", adj_neur, delimiter=', ', fmt='%12.8f')\n",
    "\n",
    "    print('saved: ',save_data)\n",
    "    return neurons, final_firing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_spike_detection(data):\n",
    "    spike_length=30 #3ms (0.003s)\n",
    "    window_length=600000 #60 sec (1min)\n",
    "    neg_data=-(data)\n",
    "    abs_data=abs(data)\n",
    "    i=0\n",
    "    ind=[]\n",
    "    while i < len(data)-window_length:\n",
    "        neg_window=neg_data[i:i+window_length]\n",
    "        abs_window=abs_data[i:i+window_length]\n",
    "        window=data[i:i+window_length]\n",
    "        #coeff=4\n",
    "        #if abso==0:\n",
    "        coeff=4\n",
    "        thresh=coeff*(scipy.stats.median_abs_deviation(window,scale='normal'))\n",
    "        #else:\n",
    "            #coeff=4\n",
    "            #thresh=coeff*(scipy.stats.median_abs_deviation(abs_window,scale='normal'))\n",
    "        ind1, peaks =find_peaks(neg_window, height=thresh,distance=spike_length)\n",
    "        del peaks\n",
    "        last=i\n",
    "        if len(ind1):\n",
    "            last=i+ind1[-1]\n",
    "        ind.extend([index + i for index in ind1])\n",
    "        i=last+spike_length #0.003 s (30ms)\n",
    "    firing_rate=len(ind)*10000/len(data)\n",
    "    print(len(ind), ' spikes detected;  ', 'firing rate: {:.2f}'.format(firing_rate),'Hz')\n",
    "    return ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df1c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
